{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"raw/audio_senp_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTokenLength(groupByDoc):\n",
    "    groupByDoc['token_length'] = groupByDoc.token_id.max()\n",
    "    return groupByDoc\n",
    "\n",
    "df = df.groupby('doc_id').apply(setTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate dependency_length for each token\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"head_token_id\"] == 0:\n",
    "        df.at[index,'dependency_length'] = 0\n",
    "    else:\n",
    "        df.at[index,'dependency_length'] = df.at[index,'token_id'] - df.at[index,'head_token_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate jumlah dependency_length for each sentence\n",
    "current_id = df.loc[0,'doc_id']\n",
    "sum_dependency_length = 0\n",
    "df[\"sum_dependency_length\"] = np.NaN\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"doc_id\"] == current_id:\n",
    "        sum_dependency_length += abs(df.at[index,\"dependency_length\"])\n",
    "        df.at[index, \"sum_dependency_length\"] = sum_dependency_length\n",
    "    else:\n",
    "        sum_dependency_length = abs(df.at[index,\"dependency_length\"])\n",
    "        current_id = row[\"doc_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Mean Dependency Distance\n",
    "df[\"mdd\"] = df.sum_dependency_length / (df.token_length - 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.to_csv(\"audio_out.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import plotly.plotly as py\n",
    "import cufflinks as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bikin algo untuk ubah posisi kata ATAU (PRIO)\n",
    "#bikin algo untuk ubah posisi root ATAU\n",
    "#ngitung average dependency dengan nge-random depedendency root nya\n",
    "#jarak dependensi antar kelas kata, WHERE central node ROOT == VERB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create a trace\n",
    "trace = go.Scatter(\n",
    "    y = df.loc[df.sum_dependency_length.notnull(),\"token_length\"], \n",
    "    x = df.loc[df.sum_dependency_length.notnull(),\"mdd\"], \n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "py.iplot(data, filename='basic-scatter')\n",
    "\n",
    "# or plot with: plot_url = py.plot(data, filename='basic-line')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#df[df.sum_dependency_length.notnull() & df.token_length < 100 & df.mdd < 10]\n",
    "#df[df.sum_dependency_length.notnull()].query('mdd < 7').to_csv('out_audio.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_sampling = pd.DataFrame()\n",
    "\n",
    "for i, doc in enumerate(df.doc_id.unique()):\n",
    "    df_sampling = df_sampling.append(df[df.doc_id == doc])\n",
    "    print(doc)\n",
    "    print(i)\n",
    "    if i > 3:\n",
    "        break\n",
    "        \n",
    "for i in 1:80\n",
    "    df_based_on_length = df[df.token_length == i]\n",
    "    for i, doc in enumerate(df_based_on_length.doc_id.unique()):\n",
    "        if i > 3:\n",
    "            df_sampling = df_sampling.append(df_based_on_length[df_based_on_length.doc_id == doc])\n",
    "        break\n",
    "\n",
    "#df_sampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_sampling = pd.DataFrame()\n",
    "\n",
    "for i in range(1,80):\n",
    "    df_based_on_length = df[df.token_length == i]\n",
    "    for i, doc in enumerate(df_based_on_length.doc_id.unique()):\n",
    "        if i < 3:\n",
    "#            print(doc)\n",
    "            df_sampling = df_sampling.append(df_based_on_length[df_based_on_length.doc_id == doc])\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column new_token_id filled with a randomized order of the token_id\n",
    "\n",
    "import random\n",
    "\n",
    "def shuffleTokenId(groupByDoc):\n",
    "    groupByDoc['new_token_id'] = random.sample(range(1,len(groupByDoc)+1), len(groupByDoc))\n",
    "    return groupByDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleAndAppend(df, reps):\n",
    "    appendedDf = pd.DataFrame()\n",
    "    for i in range(reps):\n",
    "        df['shuffle_id'] = df['doc_id'] + \"-\" + str(i)\n",
    "        appendedDf = appendedDf.append(df.groupby('doc_id').apply(shuffleTokenId))\n",
    "    return appendedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column prev_rnd_head_token_id filled with a lookup value of the previous head_token_id from rnd_pos_id\n",
    "def setPrev_head_token_id(group):\n",
    "    for i, irow in group.iterrows():\n",
    "        for j, jrow in group.iterrows():\n",
    "            if irow[\"new_token_id\"] == jrow[\"token_id\"]:\n",
    "                group.at[i, \"prev_head_token_id\"] = jrow[\"head_token_id\"]\n",
    "                break  \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setNew_head_token_id(group):\n",
    "    for i, irow in group.iterrows():\n",
    "        for j, jrow in group.iterrows():\n",
    "            if irow[\"prev_head_token_id\"] == jrow[\"new_token_id\"]:\n",
    "                group.at[i, \"new_head_token_id\"] = jrow[\"token_id\"]\n",
    "                break\n",
    "            else:\n",
    "                group.at[i, \"new_head_token_id\"] = 0\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate new dependency_length after shuffling\n",
    "def calculateNewDependencyLength(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"new_head_token_id\"] == 0:\n",
    "            df.at[index,'new_dependency_length'] = 0\n",
    "        else:\n",
    "            df.at[index,'new_dependency_length'] = df.at[index,'token_id'] - df.at[index,'new_head_token_id']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate jumlah dependency_lenght for each sentence\n",
    "def calculateNewSumDependencyLength(group):\n",
    "    group['new_sum_dependency_length'] = np.sum(abs(group['new_dependency_length']))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleAndCalculate(df):\n",
    "    df = df.groupby('doc_id').apply(shuffleTokenId) \n",
    "    df = df.groupby('doc_id').apply(setPrev_head_token_id)\n",
    "    df = df.groupby('doc_id').apply(setNew_head_token_id)\n",
    "    df = calculateNewDependencyLength(df)\n",
    "    df = df.groupby('doc_id').apply(calculateNewSumDependencyLength)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatShuffle(df, reps):\n",
    "    df_new = pd.DataFrame()\n",
    "    for i in range(reps):\n",
    "        temp = shuffleAndCalculate(df)\n",
    "        temp[\"shuffle_id\"] = str(i)\n",
    "        temp.doc_id = temp.doc_id + \"-s\" + temp.shuffle_id \n",
    "        df_new = df_new.append(temp)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#85 docs\n",
    "repeatShuffle(df, 100).to_csv(\"allshuffle100_out.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.loc[:,[\"doc_id\",\"token_length\",\"token_id\", \"head_token_id\",\"dependency_length\",\"sum_dependency_length\",\"new_token_id\",\"prev_head_token_id\",\"new_head_token_id\", \"new_dependency_length\", \"new_sum_dependency_length\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.head(997).groupby('doc_id').doc_id.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
